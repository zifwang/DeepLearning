# Problem 5: Lienar Classier: Human vs Computer random sequences
# Author: Zifan Wang

import numpy as np
import h5py
import matplotlib.pyplot as plt

'''
    1.Use the le binary_random_sp2019.hdf5. This le has all of the data generated by
    students in HW1 and also has an equal number of sequences generated using numpy.
    Using this data, construct the (2020) matrix ^R - i.e., the sample correlation matrix
    for the data. Note that the data has been converted from 0 and 1 to 1.
'''
# Load data file to binary_random
binary_random = h5py.File('../dataset5/binary_random_sp2019.hdf5','r+')
# This data file has two keys human and machine.
humanData = np.asarray(binary_random['human'])                          # Get human generate random data with size (2400,20)
computerData = np.asarray(binary_random['machine'])                     # Get computer generate random data with size (2400,20)                         
# Create sample correlation matrix with size 20*20
Rh = humanData.T @ humanData / 2400
Rm = computerData.T @ computerData / 2400
R_tot = ( Rh + Rm ) / 2

'''
    2. 
    a. Find the eigen-vectors and eigen-values for ^R. 
    b. What is the variance of the most significant two components of the data? 
    c. What percentage of the total variance is captured by these two components? 
    d. Can you see any signicance in the eigen-vectors e0 and e1 that would suggest why they capture much of the variation? 
    e. Plot numbadk vs. k on a stem plot.
'''
# a. Find the eigen-vectors and eigen-values for ^R. 
eValues, eVectors = np.linalg.eig(R_tot)                       # Get eigen-vectors and eigen-values for correlationHuman matrix
# b. What is the variance of the most significant two components of the data?
eValuesTwo = eValues[0:2]
eVectsTwo = eVectors[0:2]                                     
# c. What percentage of the total variance is captured by these two components? 
totalVariance = np.sum(eValuesTwo)                                
topTwo = np.sum(eValuesTwo)                                   
percentageTopTwo = topTwo/totalVariance              
# d. Can you see any signicance in the eigen-vectors e0 and e1 that would suggest why they capture much of the variation? 
# print(eVectTwoHuman)
'''
    Ans: These eigen-vectors create lines which seperate data point well.
'''
# e. Plot numbadkk vs. k on a stem plot.
# plt.figure()
# plt.stem(eValues, linefmt='-')
# plt.title('Eigen-values Stem Plot')
# plt.show()

'''
    3. Create label data to indicate human or computer, i.e., computer: y = +1, human: -1 and merge the two data sets. 
       Compute the linear classier weight vector w that maps the (20*1) vector to an estimate of the label. 
       What is the error rate when you threshold ^y to a hard decision?
'''
humanOutput = -1*np.ones((humanData.shape[0],1))                                # Create human label: -1
computerOutput = np.ones((computerData.shape[0],1))                             # Create computer label: 1
Output = np.append(humanOutput,computerOutput)                                  # Merge Output data array along row: size(4800,1)
Input = np.vstack((humanData,computerData))                                     # Merge Input data array along row: size(4800,1)
weight, Re, rank, singular_vals = np.linalg.lstsq(Input, Output, rcond=None)    # Linear Classifier
# print(weight)                                                                 # See weight
Output_hat_soft = np.dot(weight,np.transpose(Input))
Output_hat_hard = np.sign(Output_hat_soft)                                      #  -1 if x < 0, 0 if x==0, 1 if x > 0
probMC = np.mean(Output!=Output_hat_hard)
print('mis classification rate: ', probMC)

'''
    4. Visualize the data and classier in two dimensions. Take a reasonable number of
    samples from each the computer and human data { e.g., 100 each. Project these onto
    e0 and e1 and plot a scatter plot of the data. Add to this plot the decision boundary
    projection in this 2D space.
'''
humanData_100 = humanData[0:200]
computerData_100 = computerData[0:200]
humanPro = np.dot(eVectsTwo,np.transpose(humanData_100))
computerPro = np.dot(eVectsTwo,np.transpose(computerData_100))
fig = plt.figure()
plt.scatter(humanPro[0], humanPro[1], label = 'human', s=2)
plt.scatter(computerPro[0], computerPro[1], label = 'machine', s=2)
plt.legend('Human Random vs. Machine Random')
axes = plt.gca()
axes.set_xlabel('e0 direction')
axes.set_ylabel('e1 direction')
plt.show()
